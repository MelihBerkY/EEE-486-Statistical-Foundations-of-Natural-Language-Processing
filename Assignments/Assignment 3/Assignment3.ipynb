{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b_WSDuHFS5yD",
        "cwYdll5gZDWL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "b_WSDuHFS5yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade nltk\n",
        "\n",
        "import nltk\n",
        "import time \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import binom\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: f'{x:.6f}')"
      ],
      "metadata": {
        "id": "CqXKG1FvcG1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HELPER FUNCTIONS**"
      ],
      "metadata": {
        "id": "4s4U-1S1ZGFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# HELPER #############\n",
        "class CustomLemmatizer:\n",
        "    tag_dict = {\"ADJ\": wordnet.ADJ,\n",
        "                \"NOUN\": wordnet.NOUN}\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def lemmatize(self, word_pos_tuple):\n",
        "        word = word_pos_tuple[0]\n",
        "        pos_tag = word_pos_tuple[1]\n",
        "        if pos_tag in self.tag_dict:\n",
        "            return self.lemmatizer.lemmatize(word, self.tag_dict[pos_tag]).lower()\n",
        "        else:\n",
        "            return word.lower()\n",
        "\n",
        "def bigram_counter(text, window_size):\n",
        "    temp_list = []\n",
        "    for i in tqdm(range(len(text) - 1)):\n",
        "        for j in range(window_size):\n",
        "            try:\n",
        "                collacation = [text[i],text[i+j+1]]\n",
        "                temp_list.append(collacation)\n",
        "            except:\n",
        "                pass\n",
        "    bigram_list, bigram_counts = np.unique(temp_list, axis=0, return_counts=True)\n",
        "    bigram_dict = {' '.join(key): value for key, value in zip(bigram_list.tolist(), bigram_counts.tolist())}\n",
        "    return bigram_list.tolist(), bigram_counts.tolist(), bigram_dict\n",
        "\n",
        "def filter_collacations(lemmatized_tokens, pos_tags, window_size, min_frequency=10):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    collocation_candidates = []\n",
        "    asd = []\n",
        "    for i in tqdm(range(len(lemmatized_tokens)-1)):\n",
        "        for j in range(window_size):\n",
        "            try:\n",
        "                bigram = (lemmatized_tokens[i], lemmatized_tokens[i+j+1])\n",
        "                asd.append(bigram)\n",
        "                pos_tag_coll = [pos_tags[i][1], pos_tags[i+j+1][1]]\n",
        "                if pos_tag_coll == ['ADJ', 'NOUN'] or pos_tag_coll == ['NOUN', 'NOUN']:\n",
        "                    # Eliminate stopwords\n",
        "                    if bigram[0].lower() not in stop_words and bigram[1].lower() not in stop_words:\n",
        "                        # Eliminate punctuations\n",
        "                        if all(char.isalpha() for char in bigram[0]) and all(char.isalpha() for char in bigram[1]):\n",
        "                            # Eliminate bigrams with frequency less than min_frequency\n",
        "                            collocation_candidates.append(bigram)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    _,_, bigram_dict = bigram_counter(lemmatized_tokens, window_size)\n",
        "    temp_col = np.unique(collocation_candidates, axis=0).tolist()\n",
        "    filtered_col = []\n",
        "    filtered_counts = []\n",
        "\n",
        "    for j, col in enumerate(temp_col):\n",
        "        if bigram_dict[\" \".join(col)] >= min_frequency:\n",
        "            filtered_col.append(col)\n",
        "            filtered_counts.append(bigram_dict[\" \".join(col)])\n",
        "\n",
        "    return filtered_col, filtered_counts\n",
        "\n",
        "def statistical_test_collacation(bigram_list_count, token_list, method, window_size=1):\n",
        "    c_w1 = []\n",
        "    c_w2 = []\n",
        "    c_w1w2 = []\n",
        "    collacation = []\n",
        "    score_list = []\n",
        "    total_token = len(token_list)\n",
        "    \n",
        "    eps = 5e-324 # math.ulp(0.0)\n",
        "    N = window_size * total_token\n",
        "\n",
        "    words, counts = np.unique(token_list, return_counts=True)\n",
        "    word_freq_dict = dict(zip(words.tolist(), counts.tolist()))\n",
        "\n",
        "    for bigram, count in bigram_list_count:\n",
        "        w1w2 = count\n",
        "        w1 = window_size * word_freq_dict[bigram[0]]\n",
        "        w2 = window_size * word_freq_dict[bigram[1]]\n",
        "        \n",
        "        if method == \"t-test\":\n",
        "            x_bar = w1w2 / N\n",
        "            mu = w1*w2 / N ** 2\n",
        "            s_2 = x_bar # actual value is x_bar * (1 - x_bar), since x_bar is very\n",
        "            # close to 0, assumed that (1 - x_bar) = 1\n",
        "            \n",
        "            test_score = (x_bar - mu) / np.sqrt(s_2 / N)\n",
        "        \n",
        "        elif method == \"chi-square test\":\n",
        "            O11 = w1w2\n",
        "            O12 = w2 - O11\n",
        "            O21 = w1 - O11\n",
        "            O22 = N - O12 - O21\n",
        "            \n",
        "            num = N * (O11*O22 - O12*O21) ** 2\n",
        "            denom = (O11 + O12) * (O11 + O21) * (O12 + O22) * (O21 + O22)\n",
        "            \n",
        "            test_score = num / denom\n",
        "            \n",
        "        elif method == \"likelihood ratio test\":\n",
        "            p = w2 / N\n",
        "            p1 = w1w2 / w1\n",
        "            p2 = (w2 - w1w2) / (N - w1)\n",
        "            \n",
        "            L_H1 = binom.pmf(w1w2, w1, p) * binom.pmf(w2-w1w2, N-w1, p)\n",
        "            L_H2 = binom.pmf(w1w2, w1, p1) * binom.pmf(w2-w1w2, N-w1, p2)\n",
        "\n",
        "            if L_H1 == 0:\n",
        "                L_H1 = eps\n",
        "            if L_H2 == 0:\n",
        "                L_H2 = eps\n",
        "            \n",
        "            test_score = -2 * np.log(L_H1 / L_H2)\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(f\"There is no {method}\")\n",
        "\n",
        "        c_w1.append(int(w1/window_size))\n",
        "        c_w2.append(int(w2/window_size))\n",
        "        c_w1w2.append(w1w2)\n",
        "        collacation.append(\" \".join(bigram))\n",
        "        score_list.append(test_score)\n",
        "\n",
        "    data = {\n",
        "        \"Bi-gram\": collacation,\n",
        "        f\"{method[:-4]}score\": score_list,\n",
        "        \"c(w1w2)\": c_w1w2,\n",
        "        \"c(w1)\": c_w1,\n",
        "        \"c(w2)\": c_w2\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df.sort_values(by=f\"{method[:-4]}score\", ascending=False)\n",
        "    df.index = pd.RangeIndex(start=1, stop=len(df) + 1, name=\"rank\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "SLtuMBYgLekt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1**"
      ],
      "metadata": {
        "id": "cwYdll5gZDWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# Part1 a #############\n",
        "t0 = time.time()\n",
        "file_path = \"Fyodor Dostoyevski Processed.txt\"\n",
        "with open(file_path, \"r\") as file:\n",
        "    text = file.read()\n",
        "elapsed_time = time.time() - t0\n",
        "print(f\"Part 1a elapsed time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "############# Part1 b #############\n",
        "t0 = time.time()\n",
        "tokenized_text = word_tokenize(text)\n",
        "elapsed_time = time.time() - t0\n",
        "print(f\"Part 1b elapsed time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "############# Part1 c #############\n",
        "t0 = time.time()\n",
        "pos_tags = pos_tag(tokenized_text, tagset=\"universal\")\n",
        "elapsed_time = time.time() - t0\n",
        "print(f\"Part 1c elapsed time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "############# Part1 d #############\n",
        "t0 = time.time()\n",
        "lemmatizer = CustomLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token_pos) for token_pos in tqdm(pos_tags)]\n",
        "elapsed_time = time.time() - t0\n",
        "print(f\"Part 1d elapsed time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "############# Part1 e #############\n",
        "t0 = time.time()\n",
        "bigrams_size_1, bigrams_size_1_counts, _ = bigram_counter(lemmatized_tokens, window_size=1)\n",
        "bigrams_size_3, bigrams_size_3_counts, _  = bigram_counter(lemmatized_tokens, window_size=3)\n",
        "elapsed_time = time.time() - t0\n",
        "print(f\"Part 1e elapsed time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "############# Part1 f #############\n",
        "t0 = time.time()\n",
        "bigrams_filtered_size_1, bigrams_filtered_size_1_counts = filter_collacations(lemmatized_tokens, pos_tags, window_size=1, min_frequency=10)\n",
        "bigrams_filtered_size_3, bigrams_filtered_size_3_counts = filter_collacations(lemmatized_tokens, pos_tags, window_size=3, min_frequency=10)\n",
        "elapsed_time = time.time() - t0\n",
        "print(f\"Part 1f elapsed time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "# Print the number of bi-grams\n",
        "print(f\"\\nThe number of bi-grams with window_size=1 is\\n\\tbefore filtering: {len(bigrams_size_1)} and after filtering: {len(bigrams_filtered_size_1)}\")\n",
        "print(f\"\\nThe number of bi-grams with window_size=3 is\\n\\tbefore filtering: {len(bigrams_size_3)} and after filtering: {len(bigrams_filtered_size_3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP6eKV1gb_01",
        "outputId": "7e3167b0-bd0a-45ae-cc5e-2d92fe6923a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1a elapsed time: 0.0561 seconds\n",
            "Part 1b elapsed time: 15.9923 seconds\n",
            "Part 1c elapsed time: 58.6910 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1425758/1425758 [00:02<00:00, 508197.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1d elapsed time: 2.8147 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1425757/1425757 [00:03<00:00, 471673.65it/s]\n",
            "100%|██████████| 1425757/1425757 [00:06<00:00, 224608.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1e elapsed time: 36.7585 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1425757/1425757 [00:01<00:00, 823034.60it/s]\n",
            "100%|██████████| 1425757/1425757 [00:02<00:00, 613840.46it/s]\n",
            "100%|██████████| 1425757/1425757 [00:04<00:00, 310450.53it/s]\n",
            "100%|██████████| 1425757/1425757 [00:05<00:00, 243648.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1f elapsed time: 41.6350 seconds\n",
            "\n",
            "The number of bi-grams with window_size=1 is\n",
            "\tbefore filtering: 280830 and after filtering: 438\n",
            "\n",
            "The number of bi-grams with window_size=3 is\n",
            "\tbefore filtering: 897060 and after filtering: 790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1 Answers**"
      ],
      "metadata": {
        "id": "SaM6ZFq0-PCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words, counts = np.unique(lemmatized_tokens, return_counts=True)\n",
        "word_count_dict = dict(zip(words.tolist(), counts.tolist()))\n",
        "\n",
        "idx1 = bigrams_size_1.index([\"magnificent\", \"capital\"])\n",
        "idx2 = bigrams_size_3.index([\"bright\", \"fire\"])"
      ],
      "metadata": {
        "id": "3qWIfllc-TgJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Part1b\")\n",
        "print(f\"What is the number of tokens in the corpus? N = {len(tokenized_text)}\")\n",
        "print(\"Part1d\")\n",
        "print(f\"Counts of word [that]   = {word_count_dict['that']}\")\n",
        "print(f\"Counts of word [the]    = {word_count_dict['the']}\")\n",
        "print(f\"Counts of word [abject] = {word_count_dict['abject']}\")\n",
        "print(f\"Counts of word [london] = {word_count_dict['london']}\")\n",
        "print(f\"Counts of word [.]      = {word_count_dict['.']}\")\n",
        "print(\"Part1e\")\n",
        "print(f\"'magnificent capital' occur in windows of size 1: {bigrams_size_1_counts[idx1]}\")\n",
        "print(f\"'bright fire' occur in windows of size 1: {bigrams_size_3_counts[idx2]}\")\n",
        "print(\"Part1f\")\n",
        "try:\n",
        "    idx3 = bigrams_filtered_size_1.index([\"mr.\", \"skimpole\"])\n",
        "    print(f\"'Mr. Skimpole' occur in windows of size 1: {bigrams_size_3_counts[idx3]}\")\n",
        "except:\n",
        "    print(\"'Mr. Skimpole' doesnt exists after filtering\")\n",
        "try:\n",
        "    idx4 = bigrams_filtered_size_3.index([\"spontaneous\", \"combustion\"])\n",
        "    print(f\"'spontaneous combustion' occur in windows of size 1: {bigrams_size_3_counts[idx4]}\")\n",
        "except:\n",
        "    print(\"'spontaneous combustion' doesnt exists after filtering\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eoEWvd_-TeI",
        "outputId": "d3b792fe-e5a3-493d-e90f-e58491de71ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part1b\n",
            "What is the number of tokens in the corpus? N = 1425758\n",
            "Part1d\n",
            "Counts of word [that]   = 19429\n",
            "Counts of word [the]    = 48392\n",
            "Counts of word [abject] = 21\n",
            "Counts of word [london] = 2\n",
            "Counts of word [.]      = 51738\n",
            "Part1e\n",
            "'magnificent capital' occur in windows of size 1: 1\n",
            "'bright fire' occur in windows of size 1: 1\n",
            "Part1f\n",
            "'Mr. Skimpole' doesnt exists after filtering\n",
            "'spontaneous combustion' doesnt exists after filtering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 2**"
      ],
      "metadata": {
        "id": "v0Y8FRI5Z-G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# t-test, Chi-Sqaure test and Likelihood Ratio Test for Collacation Candidates with window_size=1\n",
        "df_size_1_t_test   = statistical_test_collacation(zip(bigrams_filtered_size_1, bigrams_filtered_size_1_counts), lemmatized_tokens, method = \"t-test\", window_size = 1)\n",
        "df_size_1_chi_test = statistical_test_collacation(zip(bigrams_filtered_size_1, bigrams_filtered_size_1_counts), lemmatized_tokens, method = \"chi-square test\", window_size = 1)\n",
        "df_size_1_mle_test = statistical_test_collacation(zip(bigrams_filtered_size_1, bigrams_filtered_size_1_counts), lemmatized_tokens, method = \"likelihood ratio test\", window_size = 1)\n",
        "\n",
        "# t-test, Chi-Sqaure test and Likelihood Ratio Test for Collacation Candidates with window_size=3\n",
        "df_size_3_t_test   = statistical_test_collacation(zip(bigrams_filtered_size_3, bigrams_filtered_size_3_counts), lemmatized_tokens, method = \"t-test\", window_size = 3)\n",
        "df_size_3_chi_test = statistical_test_collacation(zip(bigrams_filtered_size_3, bigrams_filtered_size_3_counts), lemmatized_tokens, method = \"chi-square test\", window_size = 3)\n",
        "df_size_3_mle_test = statistical_test_collacation(zip(bigrams_filtered_size_3, bigrams_filtered_size_3_counts), lemmatized_tokens, method = \"likelihood ratio test\", window_size = 3)"
      ],
      "metadata": {
        "id": "gatiGZitbGa4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Statistical Test Analysis for Bigrams with window_size=1\")\n",
        "print(df_size_1_t_test.head(20))\n",
        "print(\"\\n\")\n",
        "print(df_size_1_chi_test.head(20))\n",
        "print(\"\\n\")\n",
        "print(df_size_1_mle_test.head(20))\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"Statistical Test Analysis for Bigrams with window_size=3\")\n",
        "print(df_size_3_t_test.head(20))\n",
        "print(\"\\n\")\n",
        "print(df_size_3_chi_test.head(20))\n",
        "print(\"\\n\")\n",
        "print(df_size_3_mle_test.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lMQlETXR9wa",
        "outputId": "781bf263-ddfc-4cc5-f997-e6a6659971ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical Test Analysis for Bigrams with window_size=1\n",
            "                      Bi-gram   t-score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                                          \n",
            "1         stepan trofimovitch 22.619069      512    525    513\n",
            "2          pyotr stepanovitch 22.547831      509    834    509\n",
            "3            varvara petrovna 20.534433      422    474    507\n",
            "4           katerina ivanovna 20.239065      410    427    635\n",
            "5     nikolay vsyevolodovitch 17.657104      312    518    312\n",
            "6           fyodor pavlovitch 17.052922      291    306    461\n",
            "7                     old man 16.857563      289   1356   2546\n",
            "8         nastasia philipovna 15.583748      243    417    251\n",
            "9                   young man 15.140569      232    776   2546\n",
            "10                  old woman 14.353161      208   1356   1047\n",
            "11           yulia mihailovna 14.175298      201    215    202\n",
            "12           pyotr petrovitch 13.100114      172    834    331\n",
            "13      lizabetha prokofievna 13.074941      171    185    177\n",
            "14                 great deal 12.790646      164   1202    237\n",
            "15        dmitri fyodorovitch 12.720228      162    427    327\n",
            "16         evgenie pavlovitch 12.563966      158    227    461\n",
            "17            thousand rouble 11.980513      144    614    543\n",
            "18                  long time 11.793031      143   1074   2623\n",
            "19       mavriky nikolaevitch 11.487925      132    149    132\n",
            "20                 first time 11.192478      130   1297   2623\n",
            "\n",
            "\n",
            "                     Bi-gram  chi-square score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                                                 \n",
            "1        stepan trofimovitch    1387729.404332      512    525    513\n",
            "2          lef nicolaievitch    1359441.767442       41     43     41\n",
            "3       ippolit kirillovitch    1359441.767442       41     43     41\n",
            "4          avdotya romanovna    1341883.294118      112    119    112\n",
            "5           yulia mihailovna    1326305.257071      201    215    202\n",
            "6            nikodim fomitch    1316082.461538       24     24     26\n",
            "7      lizabetha prokofievna    1273170.747543      171    185    177\n",
            "8       mavriky nikolaevitch    1263072.563758      132    149    132\n",
            "9        trifon borissovitch    1235651.733333       39     45     39\n",
            "10        rodion romanovitch    1205267.278351       82     97     82\n",
            "11        mihail makarovitch    1197633.360000       21     25     21\n",
            "12    gavrila ardalionovitch    1173527.382417       58     61     67\n",
            "13          arina prohorovna    1120124.358458       39     44     44\n",
            "14          varvara petrovna    1056419.240946      422    474    507\n",
            "15       semyon yakovlevitch    1034363.294118       37     51     37\n",
            "16            kuzma kuzmitch     983275.172414       20     29     20\n",
            "17          daria alexeyevna     980371.988566       19     21     25\n",
            "18            darya pavlovna     935805.630069       49     62     59\n",
            "19         katerina ivanovna     883756.302566      410    427    635\n",
            "20        pyotr stepanovitch     869958.509592      509    834    509\n",
            "\n",
            "\n",
            "                      Bi-gram  likelihood ratio score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                                                        \n",
            "1           avdotya romanovna             1484.988324      112    119    112\n",
            "2          rodion romanovitch             1484.485695       82     97     82\n",
            "3        mavriky nikolaevitch             1484.274974      132    149    132\n",
            "4          andrey antonovitch             1483.464043       83    142     83\n",
            "5                   de griers             1482.991266       95    243     95\n",
            "6        nikolay parfenovitch             1482.609155      103    518    103\n",
            "7         stepan trofimovitch             1482.524036      512    525    513\n",
            "8            yulia mihailovna             1482.442392      201    215    202\n",
            "9     nikolay vsyevolodovitch             1482.215735      312    518    312\n",
            "10         pyotr stepanovitch             1481.769448      509    834    509\n",
            "11      lizabetha prokofievna             1480.829440      171    185    177\n",
            "12     pulcheria alexandrovna             1480.272014      123    124    242\n",
            "13            madame hohlakov             1479.994841       90    247     93\n",
            "14        nastasia philipovna             1478.483150      243    417    251\n",
            "15          fyodor pavlovitch             1477.400558      291    306    461\n",
            "16          katerina ivanovna             1476.984074      410    427    635\n",
            "17           varvara petrovna             1476.922372      422    474    507\n",
            "18                 great deal             1475.957207      164   1202    237\n",
            "19        alexey fyodorovitch             1475.772277      106    226    327\n",
            "20         evgenie pavlovitch             1475.616140      158    227    461\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Statistical Test Analysis for Bigrams with window_size=3\n",
            "                      Bi-gram   t-score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                                          \n",
            "1         stepan trofimovitch 22.602372      512    525    513\n",
            "2          pyotr stepanovitch 22.521437      509    834    509\n",
            "3            varvara petrovna 20.518023      422    474    507\n",
            "4           katerina ivanovna 20.220280      410    427    635\n",
            "5     nikolay vsyevolodovitch 17.644269      312    518    312\n",
            "6           fyodor pavlovitch 17.041322      291    306    461\n",
            "7                     old man 16.602812      290   1356   2546\n",
            "8         nastasia philipovna 15.574329      243    417    251\n",
            "9                   young man 15.025297      234    776   2546\n",
            "10                  old woman 14.459145      215   1356   1047\n",
            "11           yulia mihailovna 14.171001      201    215    202\n",
            "12      lizabetha prokofievna 13.071428      171    185    177\n",
            "13           pyotr petrovitch 13.070587      172    834    331\n",
            "14                 great deal 12.798568      165   1202    237\n",
            "15        dmitri fyodorovitch 12.704839      162    427    327\n",
            "16         evgenie pavlovitch 12.552288      158    227    461\n",
            "17                      ha ha 12.519300      157    252    252\n",
            "18            thousand rouble 12.473976      157    614    543\n",
            "19             hundred rouble 12.329783      153    428    543\n",
            "20                     let go 11.689332      145   1085   1858\n",
            "\n",
            "\n",
            "                     Bi-gram  chi-square score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                                                 \n",
            "1        stepan trofimovitch     461893.888881      512    525    513\n",
            "2          lef nicolaievitch     453092.589736       41     43     41\n",
            "3       ippolit kirillovitch     453092.589736       41     43     41\n",
            "4          avdotya romanovna     447145.102569      112    119    112\n",
            "5           yulia mihailovna     441833.767361      201    215    202\n",
            "6            nikodim fomitch     438662.154064       24     24     26\n",
            "7      lizabetha prokofievna     424162.261156      171    185    177\n",
            "8       mavriky nikolaevitch     420848.195141      132    149    132\n",
            "9        trifon borissovitch     411831.911773       39     45     39\n",
            "10        rodion romanovitch     401646.429202       82     97     82\n",
            "11        mihail makarovitch     399183.120205       21     25     21\n",
            "12    gavrila ardalionovitch     391098.462460       58     61     67\n",
            "13          arina prohorovna     373322.786984       39     44     44\n",
            "14          varvara petrovna     351577.190553      422    474    507\n",
            "15       semyon yakovlevitch     344738.432225       37     51     37\n",
            "16            kuzma kuzmitch     327731.724412       20     29     20\n",
            "17          daria alexeyevna     326765.329780       19     21     25\n",
            "18                  wisp tow     323415.555698       14     18     16\n",
            "19            darya pavlovna     311869.878594       49     62     59\n",
            "20         katerina ivanovna     294038.909659      410    427    635\n",
            "\n",
            "\n",
            "                      Bi-gram  likelihood ratio score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                                                        \n",
            "1           avdotya romanovna             1475.449377      112    119    112\n",
            "2        mavriky nikolaevitch             1475.092831      132    149    132\n",
            "3       lizabetha prokofievna             1474.542984      171    185    177\n",
            "4      pulcheria alexandrovna             1474.390261      123    124    242\n",
            "5            yulia mihailovna             1474.269767      201    215    202\n",
            "6                       ha ha             1473.984641      157    252    252\n",
            "7                  great deal             1473.841844      165   1202    237\n",
            "8         nastasia philipovna             1473.692388      243    417    251\n",
            "9         dmitri fyodorovitch             1473.543022      162    427    327\n",
            "10           pyotr petrovitch             1473.416282      172    834    331\n",
            "11         evgenie pavlovitch             1473.293795      158    227    461\n",
            "12    nikolay vsyevolodovitch             1473.248923      312    518    312\n",
            "13             hundred rouble             1473.002539      153    428    543\n",
            "14          fyodor pavlovitch             1472.915994      291    306    461\n",
            "15           varvara petrovna             1472.508886      422    474    507\n",
            "16        stepan trofimovitch             1472.424324      512    525    513\n",
            "17         pyotr stepanovitch             1472.273630      509    834    509\n",
            "18          katerina ivanovna             1472.263746      410    427    635\n",
            "19                    old man             1470.707417      290   1356   2546\n",
            "20                  young man             1457.108826      234    776   2546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 3**"
      ],
      "metadata": {
        "id": "a8feA0JaW55H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx1 = bigrams_filtered_size_1.index([\"head\", \"clerk\"])\n",
        "idx2 = bigrams_filtered_size_1.index([\"great\", \"man\"])\n",
        "\n",
        "count1 = bigrams_filtered_size_1_counts[idx1]\n",
        "count2 = bigrams_filtered_size_1_counts[idx2]\n",
        "\n",
        "bigrams_list = [[\"head\", \"clerk\"], [\"great\", \"man\"]]\n",
        "bigrams_count = [count1, count2]\n",
        "\n",
        "# t-test, Chi-Sqaure test and Likelihood Ratio Test for 2 Specific Collacation with window_size=1\n",
        "df_size_1_t_test_sub   = statistical_test_collacation(zip(bigrams_list, bigrams_count), lemmatized_tokens, method = \"t-test\", window_size = 1)\n",
        "df_size_1_chi_test_sub = statistical_test_collacation(zip(bigrams_list, bigrams_count), lemmatized_tokens, method = \"chi-square test\", window_size = 1)\n",
        "df_size_1_mle_test_sub = statistical_test_collacation(zip(bigrams_list, bigrams_count), lemmatized_tokens, method = \"likelihood ratio test\", window_size = 1)"
      ],
      "metadata": {
        "id": "6mGzsMcJW5hS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Statistical Test Analysis for Subset of Bigrams with window_size=1\")\n",
        "print(\"\\n\")\n",
        "print(df_size_1_t_test_sub)\n",
        "print(\"\\n\")\n",
        "print(df_size_1_chi_test_sub)\n",
        "print(\"\\n\")\n",
        "print(df_size_1_mle_test_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHC0wgwdW5ey",
        "outputId": "cbe82af6-c02e-476d-f15d-14b6ddf9e38e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical Test Analysis for Subset of Bigrams with window_size=1\n",
            "\n",
            "\n",
            "         Bi-gram  t-score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                            \n",
            "1     head clerk 4.674126       22    801    136\n",
            "2      great man 3.736722       18   1202   2546\n",
            "\n",
            "\n",
            "         Bi-gram  chi-square score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                                     \n",
            "1     head clerk       6294.821739       22    801    136\n",
            "2      great man        117.403486       18   1202   2546\n",
            "\n",
            "\n",
            "         Bi-gram  likelihood ratio score  c(w1w2)  c(w1)  c(w2)\n",
            "rank                                                           \n",
            "1     head clerk              209.662656       22    801    136\n",
            "2      great man               45.158788       18   1202   2546\n"
          ]
        }
      ]
    }
  ]
}